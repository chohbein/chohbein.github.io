<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Cristian Hohbein</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    body {
      font-family: system-ui, -apple-system, BlinkMacSystemFont, sans-serif;
      max-width: 800px;
      margin: 40px auto;
      padding: 0 16px;
      line-height: 1.6;
      color: #111;
      background-color: #e6dac1;
    }
    h1 { margin-bottom: 0; }
    h2 { margin-top: 4px; font-weight: normal; color: #444; }
    a { color: #0366d6; text-decoration: none; }
    a:hover { text-decoration: underline; }
  </style>
</head>
<body>
  <h1>Cristian Hohbein</h1>
  <h2>Machine Learning Engineer</h2>

  <p>
    I build end-to-end machine learning systems, from data ingestion and modeling
    to deployment and evaluation.
  </p>

  <h3>Projects</h3>
<section class="project">
  <h4>News Article Aggregation Website</h4>
  <p class="tech">
    Python · Javascript · SQL · AWS · NLP · webscraping · multi-document summarizing · keyword extraction · database management
  </p>
  <ul style="list-style-type: disc; margin-left: 20px;">
    <li>This passion project of mine is an end-to-end date pipeline to webscrape news articles from various sources, cluster similar articles, extract keywords, and summarize each article cluster into a concise paragraph. </li>
    <li>Scraping was done with Selenium, orchestrated by AWS Lambda + EC2 with a relational AWS RDS database for storage. </li>
    <li>Keyword extraction used a weighted ensemble of Part-of-Speech + Named Entity recognition (spaCy), Semantic-Relevance (KeyBERT), and manually-included common keywords.</li>
    <li>The article clustering process was decided by testing 2 different approaches:  <a href="https://aclanthology.org/W00-0403.pdf">Centroid-Based Summarization of Multiple Documents</a> vs
    <a href="https://arxiv.org/pdf/2110.08499">PRIMERA: Pyramid-based Masked Sentence Pre-Training for Multi-doc Summarizing</a>. Our testing found that PRIMARA gave better summaries; see the <a href="https://github.com/chohbein/newsapp/blob/main/README.md">project repo</a> for more info on my process. </li>
  </ul>
  <video width="640" height="360" controls>
  <source src="news_site_vid.mov" type="video/mp4">
  Your browser does not support the video tag.
</video>
</section>

<section class="project">
  <h4>Histopathologic Cancer Detection</h4>
  <p class="tech">Python · CNN · GPU Training</p>
  <ul style="list-style-type: disc; margin-left: 20px;">
  <li>This is my final project for a deep learning course. I compared 2 different CNN architectures to predict the presence of tumors in histopathologic images.</li>
  <li>One of the major achievements I took from this project was the performance tuning I made to work within the limits of my poor little laptop GPU. </li>
  <li>Initially, I was hit with long overheating and long training times. I resolved this with techniques such as mixed precision, multiple workers, image downsizing, and batch-size tuning.</li>
  <li>I approached model architecture by testing 2 different hypotheses: Because my visual analysis of the images showed no noticable indications of cancerous features, I suspected a model focused on fine-grained details would work best.</li>
  <li>So, I built my first iteration with this hypothesis in mind: 
    <ul>
      <li>Small kernel sizes (3x3,1x1) to capture first spacial relationships then also refined features without losing spacial resolution.</li>
      <li>Minimal pooling to maintain detail. One pooling layer just to lessen computations.</li>
      <li>Padding = 1 to maintain edge information.</li>
      <li>Light dropout (0.3) to reduce overfitting while also maintaining its capacity to recognize subtle patterns.</li>
    </ul></li>
    <li>My second hypothesis was the counter to the first: a focus on broad trends in the images will give better results.
      <ul>
        <li>4x (5x5) kernels</li>
        <li>Aggressive pooling: maxpool after every conv layer to emphasize broader features.</li>
        <li>Higher dropout (0.5) to help broad-feature neurons from learning noise.
      </ul>
    </li>
  </ul>
  <h7>Final improving steps</h7>
  <p class="tech">
    <ul>
      <li>Early stopping</li>
      <li>LR Scheduling</li>
      <li>Image alteration</li>
    </ul>
  </p>
  <b>Results</b>
  <p class="tech">
  Our model focusing on fine details within the images scored better. Thus, we accept our hypothesis.
  After further improving steps, we concluded with the following:
  <br>
  <br>
  
  ROC-AUC:
  0.9813459611111561
  <br>
  [23973 2300]<br>
  [883 16849]
  <br>
  Prioritized false-negative rate down to 2%
  </p>
</section>

<section class="project">
  <h4>Project 3</h4>
  <p class="tech">Whatever tech</p>
  <p>mhm</p>
</section>

  <h3>Skills</h3>
  <p>
    Python · SQL · scikit-learn · XGBoost · NLP · AWS · PostgreSQL
  </p>

  <h3>Links</h3>
  <ul>
    <li><a href="https://github.com/chohbein">GitHub</a></li>
  </ul>
</body>
</html>
